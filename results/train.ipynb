{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already trained\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from utils.dataset import MyDataset\n",
    "from utils.tokenizer import Tokenizer\n",
    "from utils.vit import SimpleViT\n",
    "from utils.model import CoCa\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup\n",
    "device = 'cpu'\n",
    "batch_size = 256\n",
    "lr = 1e-4\n",
    "num_epochs = 2\n",
    "max_length = 23\n",
    "contrastive_loss_weight = 0.0\n",
    "caption_loss_weight = 1.0\n",
    "DATA_DIR = '../data/'\n",
    "RESULTS_DIR = os.path.join('./models_temp/', f'contrastive{str(contrastive_loss_weight)}')\n",
    "\n",
    "# Make tokenizer\n",
    "tokenizer = Tokenizer().load_vocab(os.path.join(DATA_DIR, 'vocab.json'))\n",
    "\n",
    "# Make datasets\n",
    "train_dataset = MyDataset(os.path.join(DATA_DIR, 'split.csv'), 'train', tokenizer, max_length)\n",
    "val_dataset = MyDataset(os.path.join(DATA_DIR, 'split.csv'), 'val', tokenizer, max_length)\n",
    "test_dataset = MyDataset(os.path.join(DATA_DIR, 'split.csv'), 'test', tokenizer, max_length)\n",
    "\n",
    "# Make model\n",
    "vit = SimpleViT(\n",
    "        seq_len = 2000,\n",
    "        patch_size = 40,\n",
    "        num_classes = None,\n",
    "        dim = 1024,\n",
    "        depth = 6,\n",
    "        heads = 8,\n",
    "        mlp_dim = 1024,\n",
    "        channels = 1,\n",
    "        dim_head = 64\n",
    "    ).to(device)\n",
    "\n",
    "model = CoCa(\n",
    "    dim = 512,\n",
    "    img_encoder = vit,\n",
    "    image_dim = 1024,\n",
    "    num_tokens = len(tokenizer.vocab),\n",
    "    unimodal_depth = 6,\n",
    "    multimodal_depth = 9,\n",
    "    dim_head = 64,\n",
    "    heads = 8,\n",
    "    caption_loss_weight = caption_loss_weight,\n",
    "    contrastive_loss_weight = contrastive_loss_weight,\n",
    ").to(device)\n",
    "\n",
    "# Make dataloaders\n",
    "train_dataloader = data_utils.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = data_utils.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = data_utils.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training Loop\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    all_train_losses = []\n",
    "    all_train_accs = []\n",
    "    all_val_losses = []\n",
    "    all_val_accs = []\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        ## Train model\n",
    "        model.train()\n",
    "        train_losses, train_accs, train_outputs = [], [], []\n",
    "        for spectrum, smiles in tqdm(train_dataloader):\n",
    "            spectrum, smiles = spectrum.to(device), smiles.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss, accuracy, outputs = model(tokenizer, text = smiles, images = spectrum, return_loss = True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            train_accs.append(accuracy)\n",
    "            train_outputs.extend(outputs)\n",
    "        print(f'Epoch: {epoch}, Loss: {sum(train_losses)/len(train_losses)}, Accuracy: {sum(train_accs)/len(train_accs)}')\n",
    "\n",
    "        ## Evaluate model\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_accs = []\n",
    "        val_outputs = []\n",
    "        with torch.no_grad():\n",
    "            for spectrum, smiles in val_dataloader:\n",
    "                spectrum, smiles = spectrum.to(device), smiles.to(device)\n",
    "                loss, accuracy, output = model(tokenizer, text = smiles, images = spectrum, return_loss = True)\n",
    "                val_losses.append(loss.item())\n",
    "                val_accs.append(accuracy)\n",
    "                val_outputs.extend(output)\n",
    "        print(f'Epoch: {epoch}, Val Loss: {sum(val_losses)/len(val_losses)}, Val Accuracy: {sum(val_accs)/len(val_accs)}')\n",
    "\n",
    "        # Save model checkpoint if more than halfway done with epochs\n",
    "        if epoch >= num_epochs // 2:\n",
    "            os.makedirs(os.path.join(RESULTS_DIR, 'checkpoints'), exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(RESULTS_DIR, 'checkpoints', f'epoch_{epoch}.pt'))\n",
    "\n",
    "        # Save outputs to JSON\n",
    "        os.makedirs(os.path.join(RESULTS_DIR, 'outputs'), exist_ok=True)\n",
    "        with open(os.path.join(RESULTS_DIR, 'outputs', f'train_outputs_epoch_{epoch}.json'), 'w') as f:\n",
    "            json.dump(train_outputs, f)\n",
    "        with open(os.path.join(RESULTS_DIR, 'outputs', f'val_outputs_epoch_{epoch}.json'), 'w') as f:\n",
    "            json.dump(val_outputs, f)\n",
    "\n",
    "        # Save losses and accuracies\n",
    "        all_train_losses.append(sum(train_losses)/len(train_losses))\n",
    "        all_train_accs.append(sum(train_accs)/len(train_accs))\n",
    "        all_val_losses.append(sum(val_losses)/len(val_losses))\n",
    "        all_val_accs.append(sum(val_accs)/len(val_accs))\n",
    "\n",
    "        with open(os.path.join(RESULTS_DIR, 'losses.json'), 'w') as f:\n",
    "            json.dump({'train': all_train_losses, 'val': all_val_losses}, f)\n",
    "        with open(os.path.join(RESULTS_DIR, 'accs.json'), 'w') as f:\n",
    "            json.dump({'train': all_train_accs, 'val': all_val_accs}, f)\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax[0].plot(all_train_losses)\n",
    "    ax[0].plot(all_val_losses)\n",
    "    ax[0].set_title('Loss')\n",
    "    ax[0].legend(['train', 'val'])\n",
    "    ax[1].plot(all_train_accs)\n",
    "    ax[1].plot(all_val_accs)\n",
    "    ax[1].set_title('Accuracy')\n",
    "    ax[1].legend(['train', 'val'])\n",
    "    plt.show();\n",
    "\n",
    "    # Save plots\n",
    "    fig.savefig(os.path.join('models/', f'contrastive{str(contrastive_loss_weight)}', 'history.png'))\n",
    "    plt.close(fig)\n",
    "\n",
    "else:\n",
    "    print('Model already trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the epoch with the highest val accuracy\n",
    "with open(os.path.join(RESULTS_DIR, 'accs.json'), 'r') as f:\n",
    "    val_accs = json.load(f)['val']\n",
    "max_epoch = val_accs.index(max(val_accs))\n",
    "\n",
    "# Load max epoch model\n",
    "model.load_state_dict(torch.load(os.path.join(RESULTS_DIR, 'checkpoints', f'epoch_{max_epoch}.pt'), map_location='cpu'))\n",
    "print(f'Loaded epoch {max_epoch + 1} model with val accuracy {max(val_accs)}') # +1 because epoch 0 is the first epoch\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(7)\n",
    "torch.manual_seed(7)\n",
    "\n",
    "# Evaluate model on test set\n",
    "test_accs = {}\n",
    "for k in [1, 5, 10, 20]:\n",
    "    # Generate outputs\n",
    "    outputs = []\n",
    "    for spectrum, smiles in tqdm(test_dataloader):\n",
    "        spectrum, smiles = spectrum.to(device), smiles.to(device)\n",
    "        output = model.generate_autoregressively(tokenizer, spectrum, smiles, max_length, num_attempts=k)\n",
    "        outputs.extend(output)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    num_correct = 0\n",
    "    for output in outputs:\n",
    "        if any([output['predicted'][i] == output['original'] for i in range(k)]):\n",
    "            num_correct += 1\n",
    "    acc = num_correct / len(outputs)\n",
    "\n",
    "    path_to_results = os.path.join(RESULTS_DIR, 'outputs', f'test_outputs_{k}_attempts.json')\n",
    "    with open(path_to_results, 'w') as f:\n",
    "        json.dump(outputs, f, indent=4)\n",
    "\n",
    "    test_accs[f'{k}_attempts'] = acc\n",
    "\n",
    "# Save test accuracies\n",
    "with open(os.path.join(RESULTS_DIR, 'accs.json'), 'r') as f:\n",
    "    accs = json.load(f)\n",
    "accs['test'] = test_accs\n",
    "with open(os.path.join(RESULTS_DIR, 'accs.json'), 'w') as f:\n",
    "    json.dump(accs, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get embeddings from last epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.53it/s]\n",
      "100%|██████████| 7/7 [00:02<00:00,  2.55it/s]\n",
      "100%|██████████| 7/7 [00:02<00:00,  2.49it/s]\n",
      "100%|██████████| 7/7 [00:02<00:00,  2.50it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from utils.dataset import MyDataset\n",
    "from utils.tokenizer import Tokenizer\n",
    "from utils.vit import SimpleViT\n",
    "from utils.model import CoCa\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Setup\n",
    "device = 'cpu'\n",
    "batch_size = 256\n",
    "lr = 1e-4\n",
    "num_epochs = 2\n",
    "max_length = 23\n",
    "contrastive_loss_weight = 0.0\n",
    "caption_loss_weight = 1.0\n",
    "DATA_DIR = '../data/'\n",
    "\n",
    "for contrastive_loss_weight in ['0.0', '0.1', '0.5', '1.0']:\n",
    "    RESULTS_DIR = os.path.join('./models/', f'contrastive{str(contrastive_loss_weight)}')\n",
    "\n",
    "    # Make tokenizer\n",
    "    tokenizer = Tokenizer().load_vocab(os.path.join(DATA_DIR, 'vocab.json'))\n",
    "\n",
    "    # Make datasets\n",
    "    torch.manual_seed(7)  # All the models should have the same order of molecules in the dataset so we can plot stuff later\n",
    "    test_dataset = MyDataset(os.path.join(DATA_DIR, 'split.csv'), 'test', tokenizer, max_length)\n",
    "    dataloader = data_utils.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Load model\n",
    "    vit = SimpleViT(\n",
    "            seq_len = 2000,\n",
    "            patch_size = 40,\n",
    "            num_classes = None,\n",
    "            dim = 1024,\n",
    "            depth = 6,\n",
    "            heads = 8,\n",
    "            mlp_dim = 1024,\n",
    "            channels = 1,\n",
    "            dim_head = 64\n",
    "        ).to(device)\n",
    "\n",
    "    model = CoCa(\n",
    "        dim = 512,\n",
    "        img_encoder = vit,\n",
    "        image_dim = 1024,\n",
    "        num_tokens = len(tokenizer.vocab),\n",
    "        unimodal_depth = 6,\n",
    "        multimodal_depth = 9,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        caption_loss_weight = caption_loss_weight,\n",
    "        contrastive_loss_weight = contrastive_loss_weight,\n",
    "    ).to(device)\n",
    "    \n",
    "    model.load_state_dict(torch.load(os.path.join(RESULTS_DIR, 'checkpoints', f'epoch_99.pt'), map_location='cpu'))\n",
    "\n",
    "    # Get embeddings\n",
    "    smiles_list = []\n",
    "    all_smiles_embeddings = []\n",
    "    all_spectrum_embeddings = []\n",
    "    for spectrum, smiles in tqdm(dataloader):\n",
    "        spectrum, smiles = spectrum.to(device), smiles.to(device)\n",
    "        smiles_embeddings, spectrum_embeddings = model.forward(tokenizer, text = smiles, images = spectrum, return_embeddings = True)\n",
    "        all_smiles_embeddings.append(smiles_embeddings)\n",
    "        all_spectrum_embeddings.append(spectrum_embeddings)\n",
    "        smiles_list.extend([tokenizer.decode(smile) for smile in smiles])\n",
    "\n",
    "    # Concatenate embeddings\n",
    "    all_smiles_embeddings = torch.cat(all_smiles_embeddings, dim=0)\n",
    "    all_spectrum_embeddings = torch.cat(all_spectrum_embeddings, dim=0)\n",
    "\n",
    "    # Save embeddings\n",
    "    os.makedirs(os.path.join(RESULTS_DIR, 'embeddings'), exist_ok=True)\n",
    "    torch.save(all_smiles_embeddings, os.path.join(RESULTS_DIR, 'embeddings', 'smiles_embeddings.pt'))\n",
    "    torch.save(all_spectrum_embeddings, os.path.join(RESULTS_DIR, 'embeddings', 'spectrum_embeddings.pt'))\n",
    "\n",
    "    # Save smiles list\n",
    "    with open(os.path.join(RESULTS_DIR, 'embeddings', 'smiles.txt'), 'w') as f:\n",
    "        f.write('\\n'.join(smiles_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_riporpmehc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
